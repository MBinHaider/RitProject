---
title: "Data-Driven Car Price Prediction: A Comprehensive Approach Using Regression Modeling and Feature Engineering"
author: "Suhil Almuhaisni - Mohammed Bin Haider - Shaika Alsuwaidi - Mahra Alattar"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r}
# === Data Understanding ===

## ==== Step 0: Load Necessary Libraries ====
cat("\n==== Step 0: Load Necessary Libraries ====\n")

# List of all required libraries
required_libraries <- c(
  "tidyr",         # Data tidying
  "ggplot2",       # Visualization
  "dplyr",         # Data manipulation
  "caret",         # Machine learning and feature selection
  "randomForest",  # Random Forest implementation
  "scales",        # Scaling and formatting in plots
  "reshape2",      # Data reshaping
  "glue",          # String interpolation
  "moments",       # Skewness and kurtosis calculations
  "data.table",    # Efficient data handling
  "RColorBrewer",  # Color palettes
  "patchwork"      # Combining ggplot objects
)

# Function to check, install, and load libraries
load_library <- function(package) {
  if (!requireNamespace(package, quietly = TRUE)) {  # Check if the package is installed
    cat(sprintf("Installing missing library: %s\n", package))
    install.packages(package, dependencies = TRUE)  # Install the package if missing
  }
  library(package, character.only = TRUE)  # Load the package
}

# Apply the function to all required libraries
invisible(lapply(required_libraries, load_library))

cat("All required libraries are loaded.\n")

# --- Step 1: Load Dataset ---
cat("\n==== Step 1: Load Dataset ====\n")

data <- tryCatch(
  {
    read.csv("car_price.csv")
  },
  error = function(e) {
    stop("Error loading dataset: ", e$message)
  }
)

cat("Dataset loaded successfully.\n")

# --- Step 2: Overview of Loaded Dataset ---
cat("\n==== Step 2: Overview of Loaded Dataset ====\n")

# Dataset dimensions
cat("\n---- Dataset Overview ----\n")
cat("Dataset Dimensions (Rows x Columns): ", dim(data), "\n")
cat("Column Names:\n", paste(names(data), collapse = ", "), "\n")

# Calculate and print dataset size in memory
data_size <- object.size(data)
cat("Approximate Data Size in Memory: ", format(data_size, units = "auto"), "\n")

# Data types and structure
cat("\n---- Data Types and Structure ----\n")
print(str(data))

# --- Step 3: Convert Data Types ---
cat("\n==== Step 3: Convert IDs and Categorical Columns to Appropriate Types ====\n")

data$car_ID <- as.factor(data$car_ID)  # Assuming car_ID is an identifier
data$fueltype <- as.factor(data$fueltype)
data$aspiration <- as.factor(data$aspiration)
data$doornumber <- as.factor(data$doornumber)
data$carbody <- as.factor(data$carbody)
data$drivewheel <- as.factor(data$drivewheel)
data$enginelocation <- as.factor(data$enginelocation)
data$enginetype <- as.factor(data$enginetype)
data$cylindernumber <- as.factor(data$cylindernumber)
data$fuelsystem <- as.factor(data$fuelsystem)

cat("Data types converted successfully.\n")

# --- Step 4: Profile the Dataset ---
cat("\n==== Step 4: Profile the Dataset ====\n")

# Summary statistics
cat("\n---- Summary Statistics ----\n")
numeric_cols <- setdiff(names(data)[sapply(data, is.numeric)], "car_ID")  # Exclude car_ID
categorical_cols <- setdiff(names(data)[sapply(data, is.factor)], "car_ID")  # Exclude car_ID

cat("\n--- Summary Statistics for Numeric Columns ---\n")
print(summary(data[numeric_cols]))

cat("\n--- Summary Statistics for Categorical Columns ---\n")
print(summary(data[categorical_cols]))

# --- Step 5: Visualize Numeric Variables ---
cat("\n==== Step 5: Visualize Numeric Variables ====\n")

plot_numeric_variable <- function(data, variable) {
  cat(glue::glue("\nDisplaying histogram for numeric variable: {variable}\n"))
  
  # Histogram
  print(
    ggplot(data, aes_string(x = variable)) +
      geom_histogram(fill = "lightblue", color = "black", bins = 30) +
      labs(title = paste("Histogram for", variable), x = variable, y = "Frequency") +
      theme_minimal()
  )
}

if (length(numeric_cols) > 0) {
  lapply(numeric_cols, function(col) plot_numeric_variable(data, col))
}

# --- Step 6: Visualize Categorical Variables ---
cat("\n==== Step 6: Visualize Categorical Variables ====\n")

plot_categorical_variable <- function(data, variable) {
  cat(glue::glue("\nDisplaying barplot for categorical variable: {variable}\n"))
  
  print(
    ggplot(data, aes_string(x = variable)) +
      geom_bar(fill = "skyblue", color = "darkblue") +
      labs(title = paste("Barplot for", variable), x = variable, y = "Count") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  )
}

if (length(categorical_cols) > 0) {
  lapply(categorical_cols, function(col) plot_categorical_variable(data, col))
}

# ---  Reflection on Dataset Profiling ---
cat("\n====  Reflection on Dataset Profiling ====\n")

cat(glue("
  1. **Dataset Size and Variables**:
     - Rows: {dim(data)[1]}, Columns: {dim(data)[2]}.
     - Approximate memory size: {format(data_size, units = 'auto')}.

  2. **Variable Visualizations**:
     - Histograms displayed for numeric variables (excluding car_ID).
     - Bar plots displayed for categorical variables (excluding car_ID).

  Profiling helps identify preprocessing needs, such as outlier handling, scaling, and encoding categorical features.
"))

```

```{r}
# === Data Preparation ===

# --- Step 1: Missing Values ---

# --- Sub-step 1.1: Identify Missing Values ---
cat("\n---- Sub-step 1.1: Identifying Missing Values ----\n")

# Calculate the number of missing values in each column
missing_values <- colSums(is.na(data))
cat("Missing values in each column:\n")
print(missing_values)

# Visualize Missing Values Before Handling
cat("\nVisualizing missing values (before handling)...\n")
missing_df <- data.frame(Column = names(missing_values), MissingCount = missing_values)
missing_df <- missing_df[missing_df$MissingCount > 0, ]  # Filter columns with missing values
if (nrow(missing_df) > 0) {
  # Bar plot for missing values before handling
  missing_plot_before <- ggplot(missing_df, aes(x = reorder(Column, -MissingCount), y = MissingCount)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "darkblue") +
    labs(
      title = "Missing Values Per Column (Before Handling)",
      x = "Column",
      y = "Number of Missing Values"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(missing_plot_before)
} else {
  cat("No missing values found in the dataset.\n")
}

# --- Sub-step 1.2: Handle Missing Values in car_ID ---
cat("\n---- Sub-step 1.2: Handling Missing Values in car_ID ----\n")

# car_ID is a unique identifier, so rows with missing car_ID values should be removed.
if (anyNA(data$car_ID)) {
  cat("Handling missing values in car_ID: Removing rows with missing car_ID values.\n")
  data <- data[!is.na(data$car_ID), ]  # Remove rows where car_ID is NA
  cat("Rows with missing car_ID values removed.\n")
} else {
  cat("No missing values found in car_ID.\n")
}

# --- Sub-step 1.3: Handle Numeric Missing Values (Median Imputation) ---
cat("\n---- Sub-step 1.3: Handling Numeric Missing Values ----\n")

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]

# Loop through each numeric column and fill missing values with the median
for (col in numeric_cols) {
  if (sum(is.na(data[[col]])) > 0) {
    median_val <- median(data[[col]], na.rm = TRUE)  # Calculate the median excluding NAs
    data[[col]][is.na(data[[col]])] <- median_val  # Replace missing values with the median
    cat(sprintf("Filled missing values in numeric column '%s' with median: %.2f\n", col, median_val))
  }
}

# --- Sub-step 1.4: Handle Categorical Missing Values (Mode Imputation) ---
cat("\n---- Sub-step 1.4: Handling Categorical Missing Values ----\n")

# Identify categorical columns (character or factor)
categorical_cols <- names(data)[sapply(data, function(x) is.character(x) || is.factor(x))]

# Loop through each categorical column and fill missing values with the mode
for (col in categorical_cols) {
  if (sum(is.na(data[[col]])) > 0) {
    mode_val <- names(which.max(table(data[[col]], useNA = "no")))  # Find the mode
    data[[col]][is.na(data[[col]])] <- mode_val  # Replace missing values with the mode
    cat(sprintf("Filled missing values in categorical column '%s' with mode: '%s'\n", col, mode_val))
  }
}

# --- Sub-step 1.5: Verify and Visualize Post-Imputation ---
cat("\n---- Sub-step 1.5: Verifying Post-Imputation ----\n")

# Recalculate the number of missing values in each column
final_missing_values <- colSums(is.na(data))
cat("Remaining missing values in each column (should be 0):\n")
print(final_missing_values)

# Visualize Missing Values After Handling
cat("\nVisualizing missing values (after handling)...\n")
final_missing_df <- data.frame(Column = names(final_missing_values), MissingCount = final_missing_values)
final_missing_df <- final_missing_df[final_missing_df$MissingCount > 0, ]
if (nrow(final_missing_df) > 0) {
  # Bar plot for missing values after handling
  missing_plot_after <- ggplot(final_missing_df, aes(x = reorder(Column, -MissingCount), y = MissingCount)) +
    geom_bar(stat = "identity", fill = "lightgreen", color = "darkgreen") +
    labs(
      title = "Missing Values Per Column (After Handling)",
      x = "Column",
      y = "Number of Missing Values"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(missing_plot_after)
} else {
  cat("All missing values have been successfully handled.\n")
}

# --- Reflection on Step 1: Missing Values ---
cat("\n==== Reflection on Step 1: Missing Values ====\n")
cat(
  "1. **Identification**:\n",
  "- Missing values were identified and visualized, allowing us to assess the extent of the issue.\n\n",
  
  "2. **Imputation Strategies**:\n",
  "- `car_ID`: Rows with missing values were removed as it is a unique identifier.\n",
  "- Numeric columns: Median imputation was used to ensure robustness to outliers.\n",
  "- Categorical columns: Mode imputation preserved the most common category.\n\n",
  
  "3. **Post-Imputation Analysis**:\n",
  "- All columns were rechecked to ensure no residual missing values.\n",
  "- Visualizations confirmed the success of the missing value handling process.\n\n",
  
  "By addressing missing values systematically, the dataset is now complete and ready for further preprocessing or modeling.\n"
)

# --- Step 2: Handle Duplicates ---
cat("\n==== Step 2: Handle Duplicates ====\n")

# Check for and remove duplicate rows
num_duplicates <- nrow(data) - nrow(dplyr::distinct(data))
cat(sprintf("Number of duplicate rows: %d\n", num_duplicates))

# Remove duplicates
data <- dplyr::distinct(data)
cat(sprintf("Number of rows after removing duplicates: %d\n", nrow(data)))

# --- Reflection on Step 2: Handling Duplicates ---
cat("\n==== Reflection on Step 2: Handling Duplicates ====\n")
cat(
  "1. **Initial Check**:\n",
  "- Identified and reported the number of duplicate rows: ", num_duplicates, ".\n\n",
  
  "2. **Handling**:\n",
  "- Removed all duplicate rows using `dplyr::distinct()`.\n",
  "- Remaining rows after duplicate removal: ", nrow(data), ".\n\n",
  
  "By addressing duplicates, the dataset is now free of redundant rows, ensuring consistency and accuracy for subsequent preprocessing.\n"
)

# --- Step 3: Handle Outliers ---
cat("\n==== Step 3: Handle Outliers ====\n")

# --- Sub-step 3.1: Identify Outliers ---
cat("\n---- Sub-step 3.1: Identifying Outliers in Numerical Columns ----\n")

# Function to calculate IQR bounds
identify_outliers <- function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  list(lower = Q1 - 1.5 * IQR, upper = Q3 + 1.5 * IQR)
}

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]

# Store original data for comparison
data_original <- data

# --- Sub-step 3.2: Handle Outliers Using Capping ---
cat("\n---- Sub-step 3.2: Handling Outliers Using Capping ----\n")

# Loop through numeric columns to cap outliers
for (col in numeric_cols) {
  bounds <- identify_outliers(data[[col]])
  outliers <- which(data[[col]] < bounds$lower | data[[col]] > bounds$upper)
  if (length(outliers) > 0) {
    data[[col]][outliers] <- pmin(pmax(data[[col]][outliers], bounds$lower), bounds$upper)
    cat(sprintf("Column: %-15s | Outliers Adjusted: %-4d | Bounds: [%.2f, %.2f]\n", 
                col, length(outliers), bounds$lower, bounds$upper))
  } else {
    cat(sprintf("Column: %-15s | Outliers: None\n", col))
  }
}

# --- Sub-step 3.3: Compare Before and After Handling ---
cat("\n---- Sub-step 3.3: Comparing Before and After Outlier Handling ----\n")

for (col in numeric_cols) {
  plot <- ggplot() +
    geom_boxplot(data = data_original, aes(x = "Original Data", y = .data[[col]]), 
                 outlier.colour = "red", fill = "lightblue", color = "darkblue") +
    geom_boxplot(data = data, aes(x = "After Capping", y = .data[[col]]), 
                 outlier.colour = "red", fill = "lightgreen", color = "darkblue") +
    labs(title = paste("Comparison of", col, "- Before and After Outlier Handling"), x = "", y = col) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14)) +
    scale_y_continuous(labels = scales::comma)
  print(plot)
}

# --- Why Use Capping for Predicting Car Prices? ---
cat("\n---- Why Use Capping for Predicting Car Prices? ----\n")
cat(
  "Capping reduces the impact of outliers while retaining rare but valid cases (e.g., luxury cars),\n",
  "ensuring the model learns from all data without losing critical information.\n"
)

# --- Reflection on Step 3: Handling Outliers ---
cat("\n==== Reflection on Step 3: Handling Outliers ====\n")
cat(
  "1. Outliers were detected using the IQR method and visualized with boxplots.\n",
  "2. Capping limited extreme values' influence while preserving data integrity.\n",
  "3. Boxplots showed the effectiveness of capping in reducing outlier impact.\n",
  "This ensures the dataset is robust and ready for analysis.\n"
)
# --- Step 4: Scaling / Normalizing Features ---
# 
# Scaling involves normalizing numeric features to a consistent range, often between 0 and 1.
# This step is critical for machine learning algorithms sensitive to feature magnitude, such as:
#   - Gradient Descent-Based Models (e.g., Linear or Logistic Regression)
#   - K-Nearest Neighbors (KNN)
#   - Support Vector Machines (SVM)
#   - Neural Networks
# 
# Min-Max Scaling transforms each feature using the formula:
#   Scaled Value = (Value - Min) / (Max - Min)
# This ensures that all features contribute equally during model training.
# 
# 
# # Identify numeric columns
# numeric_cols <- names(data)[sapply(data, is.numeric)]
# 
# # Apply Min-Max Scaling to Numeric Columns
# cat("\n---- Scaling Numeric Columns ----\n")
# scaled_data <- data  # Create a copy of the dataset for scaling
# scaled_data[numeric_cols] <- lapply(data[numeric_cols], function(col) {
#   scaled_col <- (col - min(col, na.rm = TRUE)) / (max(col, na.rm = TRUE) - min(col, na.rm = TRUE))
#   return(scaled_col)
# })
# cat("Features scaled using Min-Max scaling.\n")
# 
# # Optional: Visualize Original vs. Scaled Data
# cat("\n---- Visualizing Original vs. Scaled Data (Optional) ----\n")
# for (col in numeric_cols) {
#   plot <- ggplot() +
#     # Original data distribution
#     geom_density(data = data, aes_string(x = col), fill = "lightblue", alpha = 0.5, color = "darkblue") +
#     # Scaled data distribution
#     geom_density(data = scaled_data, aes_string(x = col), fill = "lightgreen", alpha = 0.5, color = "darkgreen") +
#     labs(
#       title = paste("Comparison of", col, "- Original vs. Scaled"),
#       x = col,
#       y = "Density"
#     ) +
#     theme_minimal() +
#     theme(
#       plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
#     )
#   print(plot)
# }

# --- Reflection on Step 4: Scaling / Normalizing Features ---
cat("\n==== Reflection on Step 4: Scaling / Normalizing Features ====\n")
cat(
  "1. Scaling was skipped because the selected machine learning algorithms (e.g., Random Forest, Gradient Boosting)\n",
  "   are tree-based models that do not rely on feature scaling.\n",
  "2. These models split data based on thresholds, making scaling unnecessary.\n",
  "3. Skipping scaling ensures computational efficiency without compromising model performance.\n",
  "4. If future models require distance-based methods (e.g., KNN or SVM), scaling can be revisited.\n"
)

# --- Step 5: Data Transformation ---
cat("\n==== Step 5: Data Transformation ====\n")

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]

# Function to calculate skewness
library(moments)  # Ensure the 'moments' package is installed
calculate_skewness <- function(column) {
  skewness(column, na.rm = TRUE)
}

# Apply Log Transformation to Highly Skewed Numeric Columns
cat("\n---- Applying Log Transformation to Highly Skewed Columns ----\n")
for (col in numeric_cols) {
  skew_val <- calculate_skewness(data[[col]])  # Calculate skewness
  cat(sprintf("Skewness of '%s': %.2f\n", col, skew_val))
  
  if (skew_val > 1 && min(data[[col]], na.rm = TRUE) > 0) {  # Apply if skewness > 1 and values are positive
    data[[paste0(col, "_log")]] <- log(data[[col]])
    cat(sprintf("Applied log transformation to '%s' (Skewness: %.2f).\n", col, skew_val))
  } else {
    cat(sprintf("Skipped log transformation for '%s' (Skewness: %.2f).\n", col, skew_val))
  }
}

# --- Visualize Original vs. Log Transformed Data ---
cat("\n---- Visualizing Original vs. Log Transformed Data ----\n")
for (col in numeric_cols) {
  log_col <- paste0(col, "_log")
  if (log_col %in% names(data)) {  # Ensure log-transformed column exists
    plot <- ggplot() +
      geom_density(data = data, aes_string(x = col), fill = "lightblue", alpha = 0.5, color = "darkblue") +
      geom_density(data = data, aes_string(x = log_col), fill = "lightgreen", alpha = 0.5, color = "darkgreen") +
      labs(
        title = paste("Comparison of", col, "- Original vs. Log Transformed"),
        x = col,
        y = "Density"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
      )
    print(plot)
  }
}

# --- Reflection on Step 5: Data Transformation ---
cat("\n==== Reflection on Step 5: Data Transformation ====\n")
cat(
  "1. Log transformation was applied to numeric columns with high skewness (Skewness > 1) and positive values.\n",
  "2. This reduces skewness, stabilizes variance, and improves linear relationships for better linear regression performance.\n",
  "3. Columns with low skewness or non-positive values were skipped to avoid unnecessary transformations or errors.\n",
  "4. Visualization confirmed the effectiveness of log transformation in normalizing distributions.\n"
)
# === Step 6: Feature Engineering ===
cat("\n==== Step 6: Feature Engineering ====\n")

# --- Sub-step 6.1: Review Existing Features ---
cat("\n---- Reviewing Existing Features ----\n")

# Display column names and summary statistics
cat("Current column names:\n")
print(names(data))
cat("\nSummary statistics:\n")
print(summary(data))

# --- Sub-step 6.2: Feature Engineering ---
cat("\n---- Engineering Features  ----\n")

# 1. **Interaction Features: Horsepower-to-Weight Ratio**
cat("\nCalculating Horsepower-to-Weight Ratio...\n")
if ("horsepower" %in% names(data) && "curbweight" %in% names(data)) {
  data$hp_to_weight <- data$horsepower / data$curbweight
  cat("Horsepower-to-weight ratio created.\n")
} else {
  cat("Skipped horsepower-to-weight ratio due to missing 'horsepower' or 'curbweight'.\n")
}

# 2. **Vehicle Segment Classification (Luxury, Standard, Economy)**
cat("\nClassifying Vehicle Segments...\n")
if ("price" %in% names(data)) {
  quantiles <- quantile(data$price, probs = c(0.33, 0.66, 1.0), na.rm = TRUE)
  data$segment <- cut(
    data$price,
    breaks = c(-Inf, quantiles[1], quantiles[2], quantiles[3]),
    labels = c("Economy", "Standard", "Luxury"),
    include.lowest = TRUE
  )
  cat("Vehicle segments classified into Economy, Standard, and Luxury.\n")
} else {
  cat("Skipped vehicle segment classification due to missing 'price'.\n")
}

# 3. **Fuel Economy Score**
cat("\nCalculating Fuel Economy Score...\n")
if ("citympg" %in% names(data) && "highwaympg" %in% names(data)) {
  data$fuel_economy <- (data$citympg + data$highwaympg) / 2
  cat("Fuel economy score created.\n")
} else {
  cat("Skipped fuel economy score due to missing 'citympg' or 'highwaympg'.\n")
}

# 4. **Brand Value**
cat("\nCreating Brand Value Feature...\n")
if ("CarName" %in% names(data)) {
  data <- data %>%
    mutate(CarBrand = tolower(gsub(" .*", "", CarName)))  # Extract brand name from CarName
  
  # Standardize brand names
  brand_corrections <- c("maxda" = "mazda", "vw" = "volkswagen", "vokswagen" = "volkswagen", 
                         "porcshce" = "porsche", "toyouta" = "toyota")
  data$CarBrand <- recode(data$CarBrand, !!!brand_corrections)
  
  # Calculate average price by brand
  brand_avg_price <- data %>%
    group_by(CarBrand) %>%
    summarise(BrandAvgPrice = mean(price, na.rm = TRUE), .groups = "drop")
  data <- data %>%
    left_join(brand_avg_price, by = "CarBrand")
  cat("Brand value feature created.\n")
} else {
  cat("Skipped brand value due to missing 'CarName'.\n")
}


# --- Sub-step 6.3: Visualizing Features ---
cat("\n---- Visualizing New Features ----\n")

library(ggplot2)

# 1. Visualize Horsepower-to-Weight Ratio
if ("hp_to_weight" %in% names(data)) {
  ggplot(data, aes(x = hp_to_weight, y = price)) +
    geom_point(color = "firebrick", alpha = 0.7) +  
    labs(
      title = "Horsepower-to-Weight Ratio vs Price",
      x = "Horsepower-to-Weight Ratio",
      y = "Price"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(color = "darkred", size = 14)) %>%
    print()
}

# 2. Visualize Vehicle Segment Distribution
if ("segment" %in% names(data)) {
  ggplot(data, aes(x = segment, fill = segment)) +
    geom_bar(color = "black", fill = c("#66c2a5", "#fc8d62", "#8da0cb")) +  
    labs(
      title = "Vehicle Segment Distribution",
      x = "Segment",
      y = "Count"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(color = "darkblue", size = 14),
      legend.position = "none"
    ) %>%
    print()
}

# 3. Visualize Fuel Economy Score
if ("fuel_economy" %in% names(data)) {
  ggplot(data, aes(x = fuel_economy, y = price)) +
    geom_point(color = "darkgreen", alpha = 0.7) +  
    labs(
      title = "Fuel Economy Score vs Price",
      x = "Fuel Economy Score",
      y = "Price"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(color = "forestgreen", size = 14)) %>%
    print()
}

# 4. Visualize Brand Value (Average Price by Brand)
if ("BrandAvgPrice" %in% names(data)) {
  ggplot(data, aes(x = reorder(CarBrand, price, median), y = price, fill = CarBrand)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.7) +  # Boxplot with outliers highlighted
    labs(
      title = "Price Distribution by Car Brand",
      x = "Car Brand",
      y = "Price"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(color = "darkblue", size = 14),
      axis.text.x = element_text(angle = 45, hjust = 1)
    ) +
    scale_fill_manual(values = rainbow(length(unique(data$CarBrand))))  # Custom rainbow palette for brands
} else {
  cat("BrandAvgPrice or CarBrand is not available for visualization.\n")
}

# --- Reflection on Step 6: Feature Engineering ---
cat("\n==== Reflection on Step 6: Feature Engineering ====\n")
cat(
  "1. Horsepower-to-Weight Ratio:\n",
  "   - Captures the relationship between engine power and vehicle weight.\n",
  "   - Indicates vehicle performance, relevant for pricing decisions.\n",
  "2. Vehicle Segments:\n",
  "   - Classified cars into Economy, Standard, and Luxury based on price quantiles.\n",
  "   - Adds domain knowledge about market positioning of vehicles.\n",
  "3. Fuel Economy Score:\n",
  "   - Combines city and highway MPG into a single efficiency metric.\n",
  "   - Provides insights into fuel consumption trends.\n",
  "4. Brand Value:\n",
  "   - Created 'BrandAvgPrice' to reflect consumer perception and reliability for each brand.\n",
  "These features ensure linear relationships, interpretability, and improved model performance for linear regression.\n"
)
# === Step 7: Visualizing Features Against Price ===
cat("\n==== Step 7: Visualizing Features Against Price ====\n")

library(ggplot2)
library(dplyr)
library(scales)

# --- Sub-step 7.1: Boxplots for Categorical Variables Against Price ---
cat("\n---- Boxplots for Categorical Variables Against Price ----\n")

# Identify categorical columns
categorical_cols <- names(data)[sapply(data, is.character)]
cat("Categorical Columns Identified:\n")
print(categorical_cols)

# Clean data for categorical boxplots
data_cleaned <- data %>%
  filter(!is.na(price)) %>%
  filter(!apply(., 1, function(row) any(is.na(row) | row == "")))

if (nrow(data_cleaned) == 0) {
  stop("Error: The dataset 'data_cleaned' is empty after removing null or empty values.")
}

# Generate boxplots for categorical variables
for (col in categorical_cols) {
  if (col == "CarName") {
    # Handle 'CarName': Visualize top 10 most frequent car names
    cat(sprintf("\nColumn '%s' has many categories. Visualizing top 10 most frequent.\n", col))
    
    # Get the top 10 most frequent CarNames
    top_cars <- data_cleaned %>%
      count(CarName) %>%
      arrange(desc(n)) %>%
      slice(1:10)
    
    # Filter data to include only top 10 CarNames
    filtered_data <- data_cleaned %>% filter(CarName %in% top_cars$CarName)
    
    # Create the boxplot
    plot <- ggplot(filtered_data, aes(x = reorder(CarName, price, FUN = median), y = price, fill = CarName)) +
      geom_boxplot(outlier.colour = "gold", outlier.shape = 16, outlier.size = 3, alpha = 0.8) +
      labs(
        title = "Price Distribution by Top 10 Most Frequent Car Names",
        x = "Car Name",
        y = "Price"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16, color = "purple"),
        axis.text.x = element_text(angle = 45, hjust = 1, color = "darkblue"),
        axis.text.y = element_text(color = "darkgreen")
      ) +
      scale_y_continuous(labels = scales::comma) +
      scale_fill_manual(values = grDevices::rainbow(10))  # Dynamic palette for top 10 categories
    
    # Print the plot
    print(plot)
  } else {
    # Standard boxplots for other categorical variables
    num_colors <- length(unique(data_cleaned[[col]]))
    plot <- ggplot(data_cleaned, aes_string(x = col, y = "price", fill = col)) +
      geom_boxplot(outlier.colour = "gold", outlier.shape = 16, outlier.size = 3, alpha = 0.8) +
      labs(
        title = paste("Price Distribution by", col),
        x = col,
        y = "Price"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16, color = "purple"),
        axis.text.x = element_text(angle = 45, hjust = 1, color = "darkblue"),
        axis.text.y = element_text(color = "darkgreen")
      ) +
      scale_y_continuous(labels = scales::comma) +
      scale_fill_manual(values = grDevices::rainbow(num_colors))  # Dynamic palette for all categories
    
    # Print the plot
    print(plot)
  }
}

cat("\n---- Why Boxplots for Categorical Variables? ----\n")
cat(
  "Boxplots help visualize price distributions across categories, highlighting medians, variability, and outliers. 
  For 'CarName', only the top 10 most frequent categories are visualized for better interpretability.\n"
)

# --- Sub-step 7.2: Scatterplots for Numerical Variables Against Price ---
cat("\n---- Scatterplots for Numerical Variables Against Price ----\n")

# Identify numerical columns
numeric_cols <- names(data_cleaned)[sapply(data_cleaned, is.numeric)]
cat("Numerical Columns Identified:\n")
print(numeric_cols)

# Generate scatterplots for numerical variables
for (col in numeric_cols) {
  if (col != "price") {
    cat(sprintf("\nCreating scatterplot for '%s' vs 'price'.\n", col))
    
    # Dynamically generate a large enough color palette for CarName
    num_colors <- length(unique(data_cleaned$CarName))
    color_palette <- grDevices::rainbow(num_colors)
    
    plot <- ggplot(data_cleaned, aes_string(x = col, y = "price")) +
      geom_point(aes(color = CarName), size = 2.5, alpha = 0.6) +  # Adjusted size and transparency for points
      geom_smooth(method = "lm", color = "red", se = FALSE, linewidth = 1) +  # Trendline for correlation
      labs(
        title = paste("Relationship Between", col, "and Price"),
        x = col,
        y = "Price"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16, color = "darkred"),
        axis.text.x = element_text(size = 10, color = "darkblue"),
        axis.text.y = element_text(size = 10, color = "darkgreen"),
        legend.position = "none"  # Remove legend for simplicity
      ) +
      scale_y_continuous(labels = scales::comma) +
      scale_x_continuous(labels = scales::comma) +
      scale_color_manual(values = color_palette)  # Apply dynamic color palette
    
    # Print the plot
    print(plot)
  }
}

cat("\n---- Why Scatterplots for Numerical Variables? ----\n")
cat(
  "Scatterplots reveal relationships, trends, and correlations between numerical variables and price. 
  Including a linear trendline helps identify significant predictors for car pricing.\n"
)

# === Step 8: Redundant Features and Feature Selection ===
cat("\n==== Step 8: Redundant Features and Feature Selection ====\n")

library(caret)  # For findCorrelation function
library(ggplot2)  # For heatmap visualization
library(dplyr)  # For data manipulation
library(reshape2)  # For reshaping correlation matrix

# --- Sub-step 8.1: Check and Remove Redundant Features ---
cat("\n---- Checking and Removing Redundant Features ----\n")

# Identify and remove explicitly redundant columns
redundant_cols <- c("Car_ID")  # Replace with known redundant column names
if (all(redundant_cols %in% names(data))) {
  data <- data %>% select(-all_of(redundant_cols))
  cat("Removed redundant features:\n")
  print(redundant_cols)
} else {
  cat("No specified redundant features found.\n")
}

# --- Sub-step 8.2: Feature Selection Based on Correlation Matrix ---
cat("\n---- Feature Selection Based on Correlation Matrix ----\n")

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]
cat("Numeric Columns Identified:\n")
print(numeric_cols)

# Ensure there are numeric columns
if (length(numeric_cols) == 0) {
  stop("Error: No numeric columns found in the dataset.")
}

# Generate the correlation matrix for numeric columns
correlation_matrix <- cor(data[numeric_cols], use = "complete.obs")

# Reshape the correlation matrix for visualization
correlation_melted <- melt(correlation_matrix)
colnames(correlation_melted) <- c("Feature1", "Feature2", "Correlation")

# Create a heatmap of the correlation matrix
heatmap_plot <- ggplot(correlation_melted, aes(x = Feature1, y = Feature2, fill = Correlation)) +
  geom_tile(color = "white") +  # Add gridlines to tiles
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", midpoint = 0, 
    limit = c(-1, 1), name = "Correlation"  # Define color scale limits and legend title
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5, color = "darkred")
  ) +
  labs(
    title = "Correlation Matrix of Numeric Features",
    x = "",
    y = ""
  )

# Print the heatmap
print(heatmap_plot)

# Find highly correlated features (correlation > 0.9)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.9, names = TRUE)

# Remove highly correlated features
if (length(highly_correlated) > 0) {
  cat("Highly correlated features identified and removed:\n")
  print(highly_correlated)
  data <- data %>% select(-all_of(highly_correlated))
} else {
  cat("No highly correlated features found.\n")
}

# --- Reflection on Step 8 ---
cat("\n==== Reflection on Step 8 ====\n")
cat(
  "1. Redundant Features:\n",
  "   - Explicitly removed columns like 'Car_ID' which do not contribute to predictive modeling.\n\n",
  "2. Correlation Matrix:\n",
  "   - Created a heatmap to visualize correlations among numeric features.\n",
  "   - Identified features with high correlation (cutoff: >0.9) and removed them to reduce multicollinearity.\n\n",
  "By performing this step, the dataset is now more refined and ready for modeling.\n"
)

# === Step 9: Export Final Dataset ===
cat("\n==== Step 9: Export Final Dataset ====\n")

# Define the file name for the final dataset
output_file <- "car_prices_preprocessed.csv"  # Descriptive name based on the context

# Export the dataset
tryCatch(
  {
    write.csv(data, output_file, row.names = FALSE)
    cat(sprintf("Final dataset successfully exported to '%s'.\n", output_file))
  },
  error = function(e) {
    cat(sprintf("Error exporting the final dataset: %s\n", e$message))
  }
)

# --- Reflection on Step 9 ---
cat("\n==== Reflection on Step 9 ====\n")
cat(
  "1. The cleaned and preprocessed dataset has been saved as a CSV file named 'car_prices_preprocessed.csv'.\n",
  "2. The file name reflects the dataset's purpose, making it easier to identify and reuse.\n",
  "3. This ensures consistency and reproducibility, providing a high-quality dataset ready for modeling or analysis.\n"
)

```

```{r }
# === Modeling ===

## ==== Step 1: Data Splitting ====
cat("\n==== Step 1: Train-Test-Validation Split ====\n")

# Ensure 'price' is numeric
data$price <- as.numeric(data$price)

# Split the data into training (70%), testing (15%), and validation (15%)
cat("\nSplitting the data into training, testing, and validation sets...\n")
set.seed(123)  # For reproducibility
train_indices <- createDataPartition(data$price, p = 0.7, list = FALSE)
train_data <- data[train_indices, ]
remaining_data <- data[-train_indices, ]

# Further split remaining data into testing and validation
test_indices <- createDataPartition(remaining_data$price, p = 0.5, list = FALSE)
test_data <- remaining_data[test_indices, ]
validation_data <- remaining_data[-test_indices, ]

cat(sprintf("Training set size: %d rows\n", nrow(train_data)))
cat(sprintf("Testing set size: %d rows\n", nrow(test_data)))
cat(sprintf("Validation set size: %d rows\n", nrow(validation_data)))

## ==== Step 2: Data Preprocessing ====
cat("\n==== Step 2: Data Preprocessing ====\n")

# Remove unnecessary columns (e.g., IDs, textual columns)
excluded_columns <- c("car_ID", "CarName", "CarBrand")  # Replace as needed
train_data <- train_data %>% select(-all_of(excluded_columns))
test_data <- test_data %>% select(-all_of(excluded_columns))
validation_data <- validation_data %>% select(-all_of(excluded_columns))

# Convert categorical variables to factors
categorical_vars <- names(train_data)[sapply(train_data, is.character)]
train_data[categorical_vars] <- lapply(train_data[categorical_vars], as.factor)
test_data[categorical_vars] <- lapply(test_data[categorical_vars], as.factor)
validation_data[categorical_vars] <- lapply(validation_data[categorical_vars], as.factor)

# Create dummy variables for categorical features
cat("\nCreating dummy variables for categorical features...\n")
dummies <- dummyVars(" ~ .", data = train_data)
train_data <- data.frame(predict(dummies, newdata = train_data))
test_data <- data.frame(predict(dummies, newdata = test_data))
validation_data <- data.frame(predict(dummies, newdata = validation_data))

# Ensure train, test, and validation datasets have the same columns
missing_cols <- setdiff(names(train_data), names(test_data))
for (col in missing_cols) {
  test_data[[col]] <- 0
  validation_data[[col]] <- 0
}
test_data <- test_data[, names(train_data)]
validation_data <- validation_data[, names(train_data)]

## ==== Step 3: Scaling Numeric Features ====
cat("\n==== Step 3: Scaling Numeric Features ====\n")

# Identify numeric columns (excluding the target variable 'price')
numeric_cols <- setdiff(names(train_data)[sapply(train_data, is.numeric)], "price")

# Apply Min-Max Scaling using training data parameters
preProcValues <- preProcess(train_data[, numeric_cols], method = c("range"))
train_data[, numeric_cols] <- predict(preProcValues, train_data[, numeric_cols])
test_data[, numeric_cols] <- predict(preProcValues, test_data[, numeric_cols])
validation_data[, numeric_cols] <- predict(preProcValues, validation_data[, numeric_cols])

cat("Features scaled using Min-Max scaling.\n")

# Remove zero or near-zero variance predictors
nzv <- nearZeroVar(train_data)
if (length(nzv) > 0) {
  cat("\nRemoving near-zero variance predictors:\n")
  print(names(train_data)[nzv])
  train_data <- train_data[, -nzv]
  test_data <- test_data[, -nzv]
  validation_data <- validation_data[, -nzv]
}

## ==== Step 4: Build Linear Regression Model ====
cat("\n==== Step 4: Build Linear Regression Model ====\n")

# Specify formula (price as the dependent variable)
formula <- price ~ .

# Train the linear regression model
linear_model <- lm(formula, data = train_data)

# Display model summary
cat("\nModel Summary:\n")
print(summary(linear_model))

## ==== Step 5: Evaluate Model Performance ====
cat("\n==== Step 5: Evaluate Model Performance ====\n")

# Predict on training, testing, and validation datasets
train_predictions <- predict(linear_model, train_data)
test_predictions <- predict(linear_model, test_data)
validation_predictions <- predict(linear_model, validation_data)

# Calculate RMSE and R-squared for training data
train_rmse <- sqrt(mean((train_predictions - train_data$price)^2))
train_r2 <- summary(linear_model)$r.squared
cat(sprintf("Training RMSE: %.2f\n", train_rmse))
cat(sprintf("Training R-squared: %.2f\n", train_r2))

# Calculate RMSE and R-squared for testing data
test_rmse <- sqrt(mean((test_predictions - test_data$price)^2))
test_r2 <- cor(test_data$price, test_predictions)^2
cat(sprintf("Testing RMSE: %.2f\n", test_rmse))
cat(sprintf("Testing R-squared: %.2f\n", test_r2))

# Calculate RMSE and R-squared for validation data
validation_rmse <- sqrt(mean((validation_predictions - validation_data$price)^2))
validation_r2 <- cor(validation_data$price, validation_predictions)^2
cat(sprintf("Validation RMSE: %.2f\n", validation_rmse))
cat(sprintf("Validation R-squared: %.2f\n", validation_r2))

## ==== Step 6: Visualize Model Results ====
cat("\n==== Step 6: Visualize Model Results ====\n")

# Residual Plot
residuals <- test_data$price - test_predictions
ggplot(data.frame(Predicted = test_predictions, Residuals = residuals), aes(x = Predicted, y = Residuals)) +
  geom_point(color = "darkblue", alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Residual Plot: Predicted vs Residuals",
    x = "Predicted Prices",
    y = "Residuals"
  ) +
  theme_minimal()

# Actual vs Predicted Prices Plot
ggplot(data.frame(Actual = test_data$price, Predicted = test_predictions), aes(x = Actual, y = Predicted)) +
  geom_point(color = "darkgreen", alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Prices",
    x = "Actual Prices",
    y = "Predicted Prices"
  ) +
  theme_minimal()

## ==== Step 7: Confusion Matrix Heatmap ====
cat("\n==== Step 7: Confusion Matrix Heatmap ====\n")

# Round predictions and actual prices for classification-like analysis
actual <- round(test_data$price)
predicted <- round(test_predictions)

conf_matrix <- table(Predicted = predicted, Actual = actual)
conf_matrix_df <- as.data.frame(conf_matrix)
ggplot(conf_matrix_df, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), color = "black") +
  scale_fill_gradient(low = "lightblue", high = "blue") +
  labs(title = "Confusion Matrix Heatmap", x = "Predicted", y = "Actual") +
  theme_minimal()

## ==== Step 8: ROC Curve ====
cat("\n==== Step 8: ROC Curve ====\n")

# Bin 'price' into categories
price_bins <- quantile(test_data$price, probs = c(0, 0.33, 0.66, 1), na.rm = TRUE)
test_data$price_bin <- cut(
  test_data$price,
  breaks = c(-Inf, price_bins[2], price_bins[3], Inf),
  labels = c("Low", "Medium", "High"),
  include.lowest = TRUE
)

# Generate ROC curve
library(pROC)
roc_curve <- multiclass.roc(as.numeric(test_data$price_bin), test_predictions)
roc_list <- roc_curve$rocs
plot(roc_list[[1]], col = "red", main = "ROC Curves for Multiclass Linear Regression")
for (i in 2:length(roc_list)) {
  plot(roc_list[[i]], add = TRUE, col = rainbow(length(roc_list))[i])
}
legend(
  "bottomright",
  legend = paste("Class", 1:length(roc_list)),
  col = rainbow(length(roc_list)),
  lty = 1
)

## ==== Step 9: Precision-Recall Curve ====
cat("\n==== Step 9: Precision-Recall Curve ====\n")

library(PRROC)
pr_curve <- pr.curve(scores.class0 = test_predictions, weights.class0 = as.numeric(test_data$price_bin == "High"), curve = TRUE)
plot(pr_curve, main = "Precision-Recall Curve for High Price Class")

# === Reflection on the Modeling Process ===
# 
# 1. **Data Splitting**: The dataset was split into training (70%), testing (15%), and validation (15%), ensuring robust performance evaluation.
# 
# 2. **Data Preprocessing**: Features were encoded, scaled, and aligned across datasets, avoiding data leakage and ensuring consistency.
# 
# 3. **Linear Regression**: The model revealed key drivers of car pricing and demonstrated reasonable predictive accuracy.
# 
# 4. **Performance Evaluation**: Metrics like RMSE and R-squared showed minimal overfitting, with validation results aligning well with testing.
# 
# 5. **Visualizations and Metrics**: Residual plots, ROC, and Precision-Recall curves identified areas for improvement, especially for high-price predictions.
# 
# **Future Improvements**:
# - Explore non-linear models and feature engineering to capture complex patterns.
# - Refine outlier handling and prediction thresholds for better class differentiation.
```
```{r}
# === Modeling and Comparison: Linear Regression vs Random Forest ===

# ==== Step 1: Data Splitting ====
cat("\n==== Step 1: Train-Test-Validation Split ====\n")

# Ensure 'price' is numeric
data$price <- as.numeric(data$price)

# Split the data into training (70%), testing (15%), and validation (15%)
set.seed(123)  # For reproducibility
train_indices <- createDataPartition(data$price, p = 0.7, list = FALSE)
train_data <- data[train_indices, ]
remaining_data <- data[-train_indices, ]

# Further split remaining data into testing and validation
test_indices <- createDataPartition(remaining_data$price, p = 0.5, list = FALSE)
test_data <- remaining_data[test_indices, ]
validation_data <- remaining_data[-test_indices, ]

cat(sprintf("Training set size: %d rows\n", nrow(train_data)))
cat(sprintf("Testing set size: %d rows\n", nrow(test_data)))
cat(sprintf("Validation set size: %d rows\n", nrow(validation_data)))

# ==== Step 2: Data Preprocessing ====
cat("\n==== Step 2: Data Preprocessing ====\n")

# Remove unnecessary columns
excluded_columns <- c("car_ID", "CarName", "CarBrand")  # Replace as needed
train_data <- train_data %>% select(-all_of(excluded_columns))
test_data <- test_data %>% select(-all_of(excluded_columns))
validation_data <- validation_data %>% select(-all_of(excluded_columns))

# Convert categorical variables to factors
categorical_vars <- names(train_data)[sapply(train_data, is.character)]
train_data[categorical_vars] <- lapply(train_data[categorical_vars], as.factor)
test_data[categorical_vars] <- lapply(test_data[categorical_vars], as.factor)
validation_data[categorical_vars] <- lapply(validation_data[categorical_vars], as.factor)

# Create dummy variables for categorical features
cat("\nCreating dummy variables for categorical features...\n")
dummies <- dummyVars(" ~ .", data = train_data)
train_data <- data.frame(predict(dummies, newdata = train_data))
test_data <- data.frame(predict(dummies, newdata = test_data))
validation_data <- data.frame(predict(dummies, newdata = validation_data))

# Ensure datasets have the same columns
missing_cols <- setdiff(names(train_data), names(test_data))
for (col in missing_cols) {
  test_data[[col]] <- 0
  validation_data[[col]] <- 0
}
test_data <- test_data[, names(train_data)]
validation_data <- validation_data[, names(train_data)]

# Scale numeric features
numeric_cols <- setdiff(names(train_data)[sapply(train_data, is.numeric)], "price")
preProcValues <- preProcess(train_data[, numeric_cols], method = c("range"))
train_data[, numeric_cols] <- predict(preProcValues, train_data[, numeric_cols])
test_data[, numeric_cols] <- predict(preProcValues, test_data[, numeric_cols])
validation_data[, numeric_cols] <- predict(preProcValues, validation_data[, numeric_cols])

# Remove near-zero variance predictors
nzv <- nearZeroVar(train_data)
if (length(nzv) > 0) {
  train_data <- train_data[, -nzv]
  test_data <- test_data[, -nzv]
  validation_data <- validation_data[, -nzv]
}

# ==== Step 3: Linear Regression ====
cat("\n==== Step 3: Linear Regression ====\n")

linear_model <- lm(price ~ ., data = train_data)
cat("\nLinear Regression Model Summary:\n")
print(summary(linear_model))

train_pred_lm <- predict(linear_model, train_data)
test_pred_lm <- predict(linear_model, test_data)
validation_pred_lm <- predict(linear_model, validation_data)

train_rmse_lm <- sqrt(mean((train_pred_lm - train_data$price)^2))
test_rmse_lm <- sqrt(mean((test_pred_lm - test_data$price)^2))
validation_rmse_lm <- sqrt(mean((validation_pred_lm - validation_data$price)^2))

train_r2_lm <- summary(linear_model)$r.squared
test_r2_lm <- cor(test_data$price, test_pred_lm)^2
validation_r2_lm <- cor(validation_data$price, validation_pred_lm)^2

cat(sprintf("Linear Regression RMSE - Train: %.2f, Test: %.2f, Validation: %.2f\n",
            train_rmse_lm, test_rmse_lm, validation_rmse_lm))

# ==== Step 4: Random Forest ====
cat("\n==== Step 4: Random Forest ====\n")
library(randomForest)

rf_model <- randomForest(price ~ ., data = train_data, ntree = 100, importance = TRUE)
cat("\nRandom Forest Model Summary:\n")
print(rf_model)

train_pred_rf <- predict(rf_model, train_data)
test_pred_rf <- predict(rf_model, test_data)
validation_pred_rf <- predict(rf_model, validation_data)

train_rmse_rf <- sqrt(mean((train_pred_rf - train_data$price)^2))
test_rmse_rf <- sqrt(mean((test_pred_rf - test_data$price)^2))
validation_rmse_rf <- sqrt(mean((validation_pred_rf - validation_data$price)^2))

train_r2_rf <- cor(train_data$price, train_pred_rf)^2
test_r2_rf <- cor(test_data$price, test_pred_rf)^2
validation_r2_rf <- cor(validation_data$price, validation_pred_rf)^2

cat(sprintf("Random Forest RMSE - Train: %.2f, Test: %.2f, Validation: %.2f\n",
            train_rmse_rf, test_rmse_rf, validation_rmse_rf))

# ==== Step 5: Comparison Metrics ====
comparison <- data.frame(
  Model = c("Linear Regression", "Random Forest"),
  RMSE_Train = c(train_rmse_lm, train_rmse_rf),
  RMSE_Test = c(test_rmse_lm, test_rmse_rf),
  RMSE_Validation = c(validation_rmse_lm, validation_rmse_rf),
  R2_Train = c(train_r2_lm, train_r2_rf),
  R2_Test = c(test_r2_lm, test_r2_rf),
  R2_Validation = c(validation_r2_lm, validation_r2_rf)
)
cat("\n==== Model Comparison ====\n")
print(comparison)

# ==== Step 6: Professional Visualizations ====

# 1. RMSE and R² Comparison (Bar Plots)
comparison_long <- comparison %>%
  pivot_longer(
    cols = starts_with("RMSE") | starts_with("R2"),
    names_to = "Metric",
    values_to = "Value"
  ) %>%
  mutate(
    Metric_Type = case_when(
      grepl("RMSE", Metric) ~ "RMSE (Error)",
      grepl("R2", Metric) ~ "R² (Variance Explained)"
    )
  )

ggplot(comparison_long, aes(x = Model, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Metric_Type, scales = "free_y", nrow = 2) +
  labs(
    title = "Model Comparison: RMSE and R²",
    x = "Model",
    y = "Value",
    fill = "Model"
  ) +
  theme_minimal(base_size = 14) +
  scale_fill_manual(values = c("Linear Regression" = "#4E79A7", "Random Forest" = "#F28E2B")) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    strip.text = element_text(face = "bold", size = 12),
    legend.position = "top"
  ) +
  geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 3.5)

# 2. Actual vs. Predicted Prices: Test Data
test_data_preds <- data.frame(
  Actual = test_data$price,
  Linear_Regression = test_pred_lm,
  Random_Forest = test_pred_rf
) %>%
  pivot_longer(cols = -Actual, names_to = "Model", values_to = "Predicted")

ggplot(test_data_preds, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.6, size = 3) +
  geom_abline(slope = 1, intercept = 0, color = "black", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Prices (Test Data)",
    subtitle = "Comparison of Prediction Accuracy for Linear Regression and Random Forest",
    x = "Actual Prices",
    y = "Predicted Prices",
    color = "Model"
  ) +
  theme_minimal(base_size = 14) +
  scale_color_manual(values = c("Linear_Regression" = "#4E79A7", "Random_Forest" = "#F28E2B")) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, face = "italic"),
    legend.position = "top"
  )

# 3. Residual Analysis: Test Data
residuals_data <- data.frame(
  Actual = test_data$price,
  Linear_Regression_Residuals = test_data$price - test_pred_lm,
  Random_Forest_Residuals = test_data$price - test_pred_rf
) %>%
  pivot_longer(
    cols = starts_with("Linear_Regression") | starts_with("Random_Forest"),
    names_to = "Model",
    values_to = "Residuals"
  )

ggplot(residuals_data, aes(x = Actual, y = Residuals, color = Model)) +
  geom_point(alpha = 0.6, size = 3) +
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  labs(
    title = "Residual Analysis: Actual Prices vs. Residuals",
    subtitle = "Evaluating Error Distribution Across Models",
    x = "Actual Prices",
    y = "Residuals",
    color = "Model"
  ) +
  theme_minimal(base_size = 14) +
  scale_color_manual(values = c("Linear_Regression_Residuals" = "#4E79A7", 
                                "Random_Forest_Residuals" = "#F28E2B")) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, face = "italic"),
    legend.position = "top"
  )

# 4. Feature Importance: Random Forest
importance_df <- data.frame(
  Feature = rownames(importance(rf_model)),
  Importance = importance(rf_model)[, 1]
) %>%
  arrange(desc(Importance))

ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "#76B7B2") +
  coord_flip() +
  labs(
    title = "Feature Importance (Random Forest)",
    subtitle = "Top Features Contributing to Car Price Prediction",
    x = "Features",
    y = "Importance Score"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, face = "italic")
  )
# ==== Step 7: Reflection ====
cat("\n==== Reflection ====\n")
cat("Linear Regression is simpler and interpretable, but its performance is limited with non-linear data.\n")
cat("Random Forest provides better accuracy across training, testing, and validation datasets.\n")
cat("However, Random Forest is less interpretable and may require more resources for training.\n")
```

